"""
This file contains TransformerEncoder, TransformerDecoder and Transformer
copied almost verbatim from the PyTorch codebase. The only change is that
the "fast path" logic in the TransformerEncoder is removed. And the src/key/memory
padding mask is removed.

"""

import torch
import torch.nn as nn
from torch import Tensor
from torch.nn import Module, ModuleList
import copy
from typing import Optional
from .te_layer import TransformerEncoderLayer
from .td_layer import TransformerDecoderLayer

# We use this for exact parity with the PyTorch implementation, having the same init
# for every layer might not be necessary.

def _get_clones(module, N):
    return ModuleList([copy.deepcopy(module) for i in range(N)])

class TransformerEncoder(nn.Module):
    def __init__(
        self,
        encoder_layer,
        num_layers: int,
        norm: Optional[nn.Module] = None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(self, 
                src: torch.Tensor, 
                mask: Optional[torch.Tensor] = None, 
                cu_seqlens: Optional[Tensor] = None, 
                max_seqlen: Optional[Tensor] = None, 
                key_padding_mask=None,
                is_causal=False):
        output = src
        for mod in self.layers:
            output = mod(output, mask, is_causal=is_causal, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen, key_padding_mask=key_padding_mask)
        if self.norm is not None:
            output = self.norm(output)
        return output


class TransformerDecoder(nn.Module):
    def __init__(
        self,
        decoder_layer,
        num_layers: int,
        norm: Optional[Module] = None,
    ):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
    
    def forward(
        self,
        tgt: Tensor,
        memory: Tensor,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_is_causal=False,
        memory_is_causal=False
    ):
        output = tgt
        for mod in self.layers:
            output = mod(
                output,
                memory,
                tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_is_causal=tgt_is_causal,
                memory_is_causal=memory_is_causal,
            )
        
        if self.norm is not None:
            output = self.norm(output)
        
        return output

class Transformer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        num_encoder_layers=6,
        num_decoder_layers=6,
        dim_feedforward=2048,
        dropout=0.1,
        activation : nn.Module = torch.nn.functional.relu,
        layer_norm_eps=1e-5,
        norm_first=True,
        bias=True,
        device='cpu',
    ):
        encoder_layer = TransformerEncoderLayer(
            d_model,
            nhead,
            dim_feedforward,
            dropout,
            activation,
            layer_norm_eps,
            batch_first=True,
            norm_first=norm_first,
            bias=bias,
            device=device,
        )
        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias, device=device)
        self.encoder = TransformerEncoder(
            encoder_layer, num_encoder_layers, encoder_norm
        )

        decoder_layer = TransformerDecoderLayer(
            d_model,
            nhead,
            dim_feedforward,
            dropout,
            activation,
            layer_norm_eps,
            bias=bias,
            device=device,
        )
        decoder_norm = nn.LayerNorm(
            d_model, eps=layer_norm_eps, bias=bias, device=device
        )
        self.decoder = TransformerDecoder(
            decoder_layer, num_decoder_layers, decoder_norm
        )

    def forward(
        self,
        src,
        tgt,
        src_mask=None,
        tgt_mask=None, 
        memory_mask=None,
        src_is_causal=False,
        tgt_is_causal=False,
        memory_is_causal=False,
    ):
        memory = self.encoder(
            src,
            mask=src_mask,
            is_causal=src_is_causal,
        )
        output = self.decoder(
            tgt,
            memory,
            tgt_mask=tgt_mask,
            memory_mask=memory_mask,
            tgt_is_causal=tgt_is_causal,
            memory_is_causal=memory_is_causal,
        )
        return output
        